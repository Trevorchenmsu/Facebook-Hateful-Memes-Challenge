{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mmf_hm_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trevorchenmsu/Facebook-Hateful-Memes-Challenge/blob/main/mmf_hm_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwCc6zN1BkIE"
      },
      "source": [
        "# MMF Colab Demo\n",
        "\n",
        "This notebook provides step-by-step instructions on how to use MMF to build new models and uses the Hateful Memes (HM) dataset for this specific tutorial.\n",
        "\n",
        "Follow these links to learn more about MMF:\n",
        "- [MMF Blog Post]()\n",
        "- [GitHub repo](https://github.com/facebookresearch/mmf)\n",
        "- [Website](https://mmf.sh) and [Documentation](https://mmf.rtfd.io)\n",
        "\n",
        "In general, the notebook demonstrates how to:\n",
        "\n",
        "1. [Download MMF](#scrollTo=l7Eo9ZqTDW3I)\n",
        "2. [Download the HM dataset](#scrollTo=nYyXt9dzEBEU&line=12&uniqifier=1)\n",
        "3. [Test pretrained models on HM](#scrollTo=nYyXt9dzEBEU&line=12&uniqifier=1)\n",
        "4. [Submit a prediction](#scrollTo=uhKvYHtWHlyr&line=3&uniqifier=1)\n",
        "5. [Train existing model on HM](#scrollTo=) \n",
        "6. [Build your model](#scrollTo=)\n",
        "7. [Train your model on HM](#scrollTo=) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Eo9ZqTDW3I"
      },
      "source": [
        "## Download MMF\n",
        "\n",
        "In this section, we will download the MMF package and required dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTvGiwu5aE91"
      },
      "source": [
        "### Prerequisites \n",
        "Please enable GPU in this notebook: Runtime > Change runtime type > Hardware Accelerator > Set to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxh_vli1Drky"
      },
      "source": [
        "First we will install the MMF package and required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrgCvqorS2j8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b321cee-ae67-4701-d35f-934d6e7157ab"
      },
      "source": [
        "!pip install --pre --upgrade mmf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mmf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/7e/7e4f3549387d6683a384f4f400aee34289914443c7d2ac60080a308182f3/mmf-1.0.0rc10-cp37-cp37m-manylinux1_x86_64.whl (404kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 16.5MB/s \n",
            "\u001b[?25hCollecting lmdb==0.98\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/5c/d56dbc2532ecf14fa004c543927500c0f645eaca8bd7ec39420c7546396a/lmdb-0.98.tar.gz (869kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 52.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf) (2.23.0)\n",
            "Collecting demjson==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/67/6db789e2533158963d4af689f961b644ddd9200615b8ce92d6cad695c65a/demjson-2.2.4.tar.gz (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.1MB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
            "\u001b[?25hCollecting torch==1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/58/668ffb25215b3f8231a550a227be7f905f514859c70a65ca59d28f9b7f60/torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0MB)\n",
            "\u001b[K     |████████████████████████████████| 752.0MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf) (0.0)\n",
            "Collecting fasttext==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf) (0.5.3)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'omegaconf' candidate (version 2.0.1rc4 at https://files.pythonhosted.org/packages/03/c6/dec84d1b2a3d645f03201dca03bc879b6116cb6503449a31d7ff9c1394a4/omegaconf-2.0.1rc4-py3-none-any.whl#sha256=e04462f7e3d8f51532221471b241f67e35a36a04e364c70987018faadd273cc0 (from https://pypi.org/simple/omegaconf/) (requires-python:>=3.6))\n",
            "Reason for being yanked: <none given>\u001b[0m\n",
            "Collecting omegaconf==2.0.1rc4\n",
            "  Downloading https://files.pythonhosted.org/packages/03/c6/dec84d1b2a3d645f03201dca03bc879b6116cb6503449a31d7ff9c1394a4/omegaconf-2.0.1rc4-py3-none-any.whl\n",
            "Collecting GitPython==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 50.3MB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ed/a894f274a7733d6492e438a5831a95b507c5ec777edf6d8c3b97574e08c4/torchvision-0.6.0-cp37-cp37m-manylinux1_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 28.0MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 45.7MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.43.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf) (3.0.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0->mmf) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf) (2.6.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf) (54.2.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/b3/8c889dd3d5ae47a9c4468cc20ef980adc4a16f06f0937ab33f78b58b5eda/boto3-1.17.53-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 58.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0->mmf) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 50.6MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.0.1rc4->mmf) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0->mmf) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf) (1.4.1)\n",
            "Collecting botocore<1.21.0,>=1.20.53\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/4e/232e261b739534e216f28d935a06c44840221c3476ebcdb411cd0fc2bf16/botocore-1.20.53-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 43.6MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/89/0cb4e92c239e6425b9b0035227b8cdf9d3d098a5c9e95632c3815df63a09/s3transfer-0.3.7-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0->mmf) (7.1.2)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.53->boto3->transformers==2.3.0->mmf) (2.8.1)\n",
            "Building wheels for collected packages: lmdb, demjson, fasttext, nltk, sacremoses\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219684 sha256=0dd299e5b73954cd74104f907afdb7700be6774b70e762c3c1db16bd4407e032\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/97/8c/7721e4b6b0ac723c6cc45ecca60599a80f75e2367330647390\n",
            "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demjson: filename=demjson-2.2.4-cp37-none-any.whl size=73546 sha256=4fa2ca3243f295e156a51cc7964cdb326a17e6766c81397bb561db3bedac235b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/d2/ab/a54fb5ea53ac3badba098160e8452fa126a51febda80440ded\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2462381 sha256=d1888db0a6c4b6e8b80d1594311c541ba027d9829fb55db41c44e0c7f4920abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449907 sha256=73a73024fb1d674bc6f6e5021ca495c378cd1e14e7371499f6e0d8b914a8f183\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=9a9525ccb3b5ded65ab06de06343a93022af593053ba44756856cb0972e5485d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built lmdb demjson fasttext nltk sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.53 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: lmdb, demjson, tqdm, sentencepiece, torch, torchtext, fasttext, jmespath, botocore, s3transfer, boto3, sacremoses, transformers, PyYAML, omegaconf, smmap, gitdb, GitPython, torchvision, nltk, mmf\n",
            "  Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed GitPython-3.1.0 PyYAML-5.4.1 boto3-1.17.53 botocore-1.20.53 demjson-2.2.4 fasttext-0.9.1 gitdb-4.0.7 jmespath-0.10.0 lmdb-0.98 mmf-1.0.0rc10 nltk-3.4.5 omegaconf-2.0.1rc4 s3transfer-0.3.7 sacremoses-0.0.44 sentencepiece-0.1.95 smmap-4.0.0 torch-1.5.0 torchtext-0.5.0 torchvision-0.6.0 tqdm-4.60.0 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYyXt9dzEBEU"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "We will now download the Hateful Memes dataset. You will require two things to download the datasets: (i) URL (ii) Password to the zip file. To get both of these follow these steps:\n",
        "\n",
        "1. Go to [DrivenData challenge page](https://www.drivendata.org/competitions/64/hateful-memes/)\n",
        "2. Register, read and acknowledge the agreements for data access.\n",
        "3. Go to the [data page](https://www.drivendata.org/competitions/64/hateful-memes/data), right click on the \"Hateful Memes challenge dataset\" link and \"Copy Link Address\" as shown in the image. This will copy the URL for the zip file to your clipboard which you will use in the next step.\n",
        "![data](https://i.imgur.com/JQx2hPm.png)\n",
        "4. Also, note the password provided in the description.\n",
        "5. Run the next code block, fill in the URL and the zipfile's password when prompted.\n",
        "\n",
        "The code blocks after that will download, convert and visualize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulosPHAE-eto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b39f5be-cb65-420c-a95b-c9144dbd6928"
      },
      "source": [
        "from getpass import getpass, getuser\n",
        "url = getpass(\"Enter the Hateful Memes data URL:\")\n",
        "password = getpass(\"Enter ZIP file's Password:\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Hateful Memes data URL:··········\n",
            "Enter ZIP file's Password:··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiux2MWzFRPz"
      },
      "source": [
        "This will actually download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Y8wI6BoNN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48366f52-700b-499c-8ee4-2425f2bf68b1"
      },
      "source": [
        "!curl -o /content/hm.zip \"$url\" -H 'Referer: https://www.drivendata.org/competitions/64/hateful-memes/data/' --compressed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4029M  100 4029M    0     0  36.1M      0  0:01:51  0:01:51 --:--:-- 36.5M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPYBxsyRFUUb"
      },
      "source": [
        "The next command will convert the zip file into required MMF format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsMmmOB3_rdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00cc2c56-7d18-4ccb-8127-7e6cef427a90"
      },
      "source": [
        "# !mmf_convert_hm --zip_file /content/hm.zip --password $password --bypass_checksum 1\n",
        "!mmf_convert_hm --zip_file=\"hm.zip\" --password=EWryfbZyNviilcDF --bypass_checksum 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-17 21:12:05.090659: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Data folder is /root/.cache/torch/mmf/data\n",
            "Zip path is hm.zip\n",
            "Moving hm.zip\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mmf_convert_hm\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mmf_cli/hm_convert.py\", line 165, in main\n",
            "    converter.convert()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mmf_cli/hm_convert.py\", line 95, in convert\n",
            "    move(src, dest)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mmf/utils/download.py\", line 424, in move\n",
            "    shutil.move(path1, path2)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 564, in move\n",
            "    raise Error(\"Destination path '%s' already exists\" % real_dst)\n",
            "shutil.Error: Destination path '/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/hm.zip' already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DASdGTrwFbL7"
      },
      "source": [
        "### Visualize\n",
        "\n",
        "The next code block will output some samples of the dataset for visualization. You can adjust number of samples, rows and size among other stuff.\n",
        "\n",
        "**Note:** *Some of the images in the hateful memes dataset are sensitive and may not be suitable for all audiences. Please run the next code responsibly keeping these conditions in mind.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "OgDytwMug2EK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "5edcd4a3-bcce-4fc6-a98b-ce8158626f8e"
      },
      "source": [
        "from mmf.common.registry import registry\n",
        "from mmf.models.mmbt import MMBT\n",
        "from mmf.utils.build import build_dataset\n",
        "\n",
        "dataset = build_dataset(\"hateful_memes\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
        "dataset.visualize(num_samples=8, size=(512, 512), nrow=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0a6cd0dec5a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hateful_memes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmf/utils/build.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(dataset_key, config, dataset_type)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmmf_typings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetBuilderType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_registry_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmf/datasets/base_dataset_builder.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(self, config, dataset_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Only build in main process, so none of the others have to build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_master\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmf/datasets/builders/hateful_memes/builder.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, config, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# NOTE: This doesn't check for files, but that is a fine assumption for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         assert PathManager.exists(test_path), (\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;34m\"Hateful Memes Dataset doesn't do automatic downloads; please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0;34m\"follow instructions at https://fb.me/hm_prerequisites\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         )\n",
            "\u001b[0;31mAssertionError\u001b[0m: Hateful Memes Dataset doesn't do automatic downloads; please follow instructions at https://fb.me/hm_prerequisites"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpjGUHXZGGRw"
      },
      "source": [
        "## Test pretrained model\n",
        "\n",
        "We will now use MMF to load an existing model MMBT to run some tests on random images from the internet. Fill in the image url and the text contained in it to see if the model thinks of it as hateful or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gFObgxu13TZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from mmf.models.mmbt import MMBT\n",
        "\n",
        "model = MMBT.from_pretrained(\"mmbt.hateful_memes.images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKzyiRYuUMYj"
      },
      "source": [
        "image_url = \"https://i.imgur.com/tEcsk5q.jpg\" #@param {type:\"string\"}\n",
        "text = \"look how many people love you\" #@param {type: \"string\"}\n",
        "output = model.classify(image_url, text)\n",
        "plt.imshow(Image.open(requests.get(image_url, stream=True).raw))\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "hateful = \"Yes\" if output[\"label\"] == 1 else \"No\"\n",
        "print(\"Hateful as per the model?\", hateful)\n",
        "print(f\"Model's confidence: {output['confidence'] * 100:.3f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhKvYHtWHlyr"
      },
      "source": [
        "## Submit a prediction\n",
        "\n",
        "Now, we will use a pretrained model from MMF to submit a prediction to DrivenData. Run the command in the next block and at the end it will output the path to the csv file generated. Download and upload that file to [DrivenData's submission page](https://www.drivendata.org/competitions/64/hateful-memes/submissions/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlPz8R7QWXIp"
      },
      "source": [
        "!mmf_predict config=projects/hateful_memes/configs/mmbt/defaults.yaml \\\n",
        "  model=mmbt \\ \n",
        "  dataset=hateful_memes \\\n",
        "  run_type=test \\ \n",
        "  checkpoint.resume_zoo=mmbt.hateful_memes.images \\\n",
        "  training.batch_size=16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDDnTlQZV46f"
      },
      "source": [
        "## Train an existing model\n",
        "\n",
        "We will use MMF to train an existing baseline from MMF's model zoo on the Hateful Memes dataset. Run the next code cell to start training MMBT-Grid model on the dataset. You can adjust the batch size, maximum number of updates, log and evaluation interval among other things by using command line overrides. Read more about MMF's configuration system at https://mmf.readthedocs.io/en/latest/notes/configuration.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nwebqtdWOfZ"
      },
      "source": [
        "!mmf_run config=projects/hateful_memes/configs/mmbt/defaults.yaml \\\n",
        "  model=mmbt \\\n",
        "  dataset=hateful_memes \\\n",
        "  training.log_interval=50 \\\n",
        "  training.max_updates=3000 \\\n",
        "  training.batch_size=16 \\\n",
        "  training.evaluation_interval=500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mB-z-6XWdBd"
      },
      "source": [
        "## Build your own model\n",
        "\n",
        "Using MMF's encoders, modules and utilities, we can easily build a custom model. In this example, we are building a fusion model which fuses ResNet pooled grid features with fasttext embedding vectors to classify a meme as hateful or not hateful. \n",
        "\n",
        "Steps involved in building the model are:\n",
        "\n",
        "1. Create a new processor to get fasttext sentence embeddings. (Read more on processors [here]())\n",
        "2. Create new model using encoders from MMF.\n",
        "3. Move hardcoded stuff from model to configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2yjX5JxIu2C"
      },
      "source": [
        "import torch \n",
        "\n",
        "# We will inherit the FastText Processor already present in MMF\n",
        "from mmf.datasets.processors import FastTextProcessor\n",
        "# registry is needed to register processor and model to be MMF discoverable\n",
        "from mmf.common.registry import registry\n",
        "\n",
        "# Register the processor so that MMF can discover it\n",
        "@registry.register_processor(\"fasttext_sentence_vector\")\n",
        "class FastTextSentenceVectorProcessor(FastTextProcessor):\n",
        "    # Override the call method\n",
        "    def __call__(self, item):\n",
        "        # This function is present in FastTextProcessor class and loads\n",
        "        # fasttext bin\n",
        "        self._load_fasttext_model(self.model_file)\n",
        "        if \"text\" in item:\n",
        "            text = item[\"text\"]\n",
        "        elif \"tokens\" in item:\n",
        "            text = \" \".join(item[\"tokens\"])\n",
        "\n",
        "        # Get a sentence vector for sentence and convert it to torch tensor\n",
        "        sentence_vector = torch.tensor(\n",
        "            self.model.get_sentence_vector(text),\n",
        "            dtype=torch.float\n",
        "        )\n",
        "\n",
        "        # Return back a dict\n",
        "        return {\n",
        "            \"text\": sentence_vector\n",
        "        }\n",
        "    \n",
        "    # Make dataset builder happy, return a random number\n",
        "    def get_vocab_size(self):\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlB4n0nwKWZn"
      },
      "source": [
        "import torch\n",
        "\n",
        "# registry is need to register our new model so as to be MMF discoverable\n",
        "from mmf.common.registry import registry\n",
        "# All model using MMF need to inherit BaseModel\n",
        "from mmf.models.base_model import BaseModel\n",
        "# ProjectionEmbedding will act as proxy encoder for FastText Sentence Vector\n",
        "from mmf.modules.embeddings import ProjectionEmbedding\n",
        "# Builder methods for image encoder and classifier\n",
        "from mmf.utils.build import build_classifier_layer, build_image_encoder\n",
        "\n",
        "# Register the model for MMF, \"concat_vl\" key would be used to find the model\n",
        "@registry.register_model(\"concat_vl\")\n",
        "class LanguageAndVisionConcat(BaseModel):\n",
        "    # All models in MMF get first argument as config which contains all\n",
        "    # of the information you stored in this model's config (hyperparameters)\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        # This is not needed in most cases as it just calling parent's init\n",
        "        # with same parameters. But to explain how config is initialized we \n",
        "        # have kept this\n",
        "        super().__init__(config, *args, **kwargs)\n",
        "    \n",
        "    # This classmethod tells MMF where to look for default config of this model\n",
        "    @classmethod\n",
        "    def config_path(cls):\n",
        "        # Relative to user dir root\n",
        "        return \"/content/hm_example_mmf/configs/models/concat_vl.yaml\"\n",
        "    \n",
        "    # Each method need to define a build method where the model's modules\n",
        "    # are actually build and assigned to the model\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Config's image_encoder attribute will used to build an MMF image\n",
        "        encoder. This config in yaml will look like:\n",
        "\n",
        "        # \"type\" parameter specifies the type of encoder we are using here. \n",
        "        # In this particular case, we are using resnet152\n",
        "        type: resnet152\n",
        "      \n",
        "        # Parameters are passed to underlying encoder class by \n",
        "        # build_image_encoder\n",
        "        params:\n",
        "          # Specifies whether to use a pretrained version\n",
        "          pretrained: true \n",
        "          # Pooling type, use max to use AdaptiveMaxPool2D\n",
        "          pool_type: avg \n",
        "      \n",
        "          # Number of output features from the encoder, -1 for original\n",
        "          # otherwise, supports between 1 to 9\n",
        "          num_output_features: 1 \n",
        "        \"\"\"\n",
        "        self.vision_module = build_image_encoder(self.config.image_encoder)\n",
        "\n",
        "        \"\"\"\n",
        "        For classifer, configuration would look like:\n",
        "        # Specifies the type of the classifier, in this case mlp\n",
        "        type: mlp\n",
        "        # Parameter to the classifier passed through build_classifier_layer\n",
        "        params:\n",
        "          # Dimension of the tensor coming into the classifier\n",
        "          in_dim: 512\n",
        "          # Dimension of the tensor going out of the classifier\n",
        "          out_dim: 2\n",
        "          # Number of MLP layers in the classifier\n",
        "          num_layers: 0\n",
        "        \"\"\"\n",
        "        self.classifier = build_classifier_layer(self.config.classifier)\n",
        "        \n",
        "        # ProjectionEmbeddings takes in params directly as it is module\n",
        "        # So, pass in kwargs, which are in_dim, out_dim and module\n",
        "        # whose value would be \"linear\" as we want linear layer\n",
        "        self.language_module = ProjectionEmbedding(\n",
        "            **self.config.text_encoder.params\n",
        "        )\n",
        "        # Dropout value will come from config now\n",
        "        self.dropout = torch.nn.Dropout(self.config.dropout)\n",
        "        # Same as Projection Embedding, fusion's layer params (which are param \n",
        "        # for linear layer) will come from config now\n",
        "        self.fusion = torch.nn.Linear(**self.config.fusion.params)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # Each model in MMF gets a dict called sample_list which contains\n",
        "    # all of the necessary information returned from the image\n",
        "    def forward(self, sample_list):\n",
        "        # Text input features will be in \"text\" key\n",
        "        text = sample_list[\"text\"]\n",
        "        # Similarly, image input will be in \"image\" key\n",
        "        image = sample_list[\"image\"]\n",
        "\n",
        "        text_features = self.relu(self.language_module(text))\n",
        "        image_features = self.relu(self.vision_module(image))\n",
        "\n",
        "        # Concatenate the features returned from two modality encoders\n",
        "        combined = torch.cat([text_features, image_features.squeeze()], dim=1)\n",
        "\n",
        "        # Pass through the fusion layer, relu and dropout\n",
        "        fused = self.dropout(self.relu(self.fusion(combined)))\n",
        "\n",
        "        # Pass final tensor from classifier to get scores\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        # For loss calculations (automatically done by MMF based on loss defined\n",
        "        # in the config), we need to return a dict with \"scores\" key as logits\n",
        "        output = {\"scores\": logits}\n",
        "\n",
        "        # MMF will automatically calculate loss\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8VCzWStDwkJ"
      },
      "source": [
        "Now, we will install the example repo that we have already created on top of MMF and contains code in this colab. We do this so that we don't have to build configs again from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjvxZYBXTrRG"
      },
      "source": [
        "!git clone https://github.com/apsdehal/hm_example_mmf /content/hm_example_mmf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taXGqCxQXJbo"
      },
      "source": [
        "## Train your model\n",
        "\n",
        "In this step, we will train the model we just built. A dot list can be passed as either a dict or a list to the run to override the configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Aci1mtsURL9"
      },
      "source": [
        "import sys\n",
        "from mmf_cli.run import run\n",
        "opts = opts=[\n",
        "    \"config='/content/hm_example_mmf/configs/experiments/defaults.yaml'\", \n",
        "    \"model=concat_vl\", \n",
        "    \"dataset=hateful_memes\", \n",
        "    \"training.num_workers=0\"\n",
        "]\n",
        "run(opts=opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTAshObTm1ey"
      },
      "source": [
        "## Using your module\n",
        "\n",
        "Since, we have cloned the repo that contains the example we built in this colab notebook we can use it also to run the training from command line by using the `env.user_dir` option or by overriding the environment variable `MMF_USER_DIR`. Expand the cell below the next code cell to see how it can be done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwggw7XLUYuO"
      },
      "source": [
        "!MMF_USER_DIR=\"/content/hm_example_mmf\" mmf_run \\\n",
        "  config=\"configs/experiments/defaults.yaml\" \\\n",
        "  model=concat_vl \\\n",
        "  dataset=hateful_memes \\\n",
        "  training.num_workers=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Y2aJC_nRpy"
      },
      "source": [
        "## Conclusion and Further Steps\n",
        "\n",
        "In this colab notebook, we learned how we can use MMF to train and predict already existing models in MMF's zoo. We also learned how we can build custom models using various modules and goodies provided in MMF easily.\n",
        "\n",
        "If you have any issues, feedback or comments, please reach us out at mmf@fb.com or open up an issue at [GitHub](https://github.com/facebookresearch/mmf/issues/new/choose). We are also accepting PRs if you want to add your cool model to MMF and we are always open to community contributions.\n",
        "\n",
        "At Facebook AI, we’ll continuously improve and expand on the multimodal capabilities available through MMF, and we welcome contributions from the community as well to build this resource. We hope MMF will be the framework of choice and be a catalyst for research in this area by providing a powerful, versatile platform for multimodal research. "
      ]
    }
  ]
}